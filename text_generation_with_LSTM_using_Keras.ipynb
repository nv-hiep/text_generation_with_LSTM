{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_with_LSTM_using_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMXk7eWozc1Kj1EqYbKO3BM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nv-hiep/text_generation_with_LSTM/blob/main/text_generation_with_LSTM_using_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPU81-ud19sb"
      },
      "source": [
        "# Text Generation With LSTM Recurrent Neural Networks using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Lyb7aGywTX"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmBY9R6OtNZS",
        "outputId": "9fc648ad-4b1f-4d58-cbff-70a5c4653ad8"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU') )\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "else:\n",
        "  device_name = '/CPU:0'\n",
        "print(device_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "WARNING:tensorflow:From <ipython-input-1-179e15cabb93>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXFyz-99yvbx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BONH6Wj3zAXh"
      },
      "source": [
        "# Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ncS6zzzFqm",
        "outputId": "a28926d3-9038-4ee8-a1ad-eea8a74291fa"
      },
      "source": [
        "! curl -O http://www.gutenberg.org/files/1268/1268-0.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1144k  100 1144k    0     0   598k      0  0:00:01  0:00:01 --:--:--  598k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVkXi8AIzUpe"
      },
      "source": [
        "with open('1268-0.txt') as datfile:\n",
        "  text = datfile.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PMZFyVSuzjFA",
        "outputId": "ba1e8123-adf4-4f18-ee0f-3c694a4e63c5"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ufeffThe Project Gutenberg EBook of The Mysterious Island, by Jules Verne\\n\\nThis eBook is for the use of '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHnZ1exO0JPE"
      },
      "source": [
        "Remove some unnecessary text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnx95GT80Igt",
        "outputId": "f154a1ea-8b00-4f4d-ef34-4158557651db"
      },
      "source": [
        "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_idx   = text.find('End of the Project Gutenberg')\n",
        "\n",
        "print('Text starts at index {}: '.format(start_idx))\n",
        "print('Text ends at index {}: '.format(end_idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text starts at index 567: \n",
            "Text ends at index 1112917: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8owJMr02Tc"
      },
      "source": [
        "text     = text[start_idx : end_idx]\n",
        "char_set = set(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrPmYWLE1b_S",
        "outputId": "7a7687c7-cef5-44c7-bfc8-8fc7d296ca5e"
      },
      "source": [
        "print(char_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'1', ';', 'w', 'S', 'm', 'j', 'n', '!', 'Z', 'd', 'F', ' ', '*', 'N', '4', 'v', 'I', 'C', 'U', '‘', 'b', 'r', 's', '’', '“', 'B', 'E', '\\n', 't', '-', 'a', 'y', 'M', '7', 'g', 'K', '”', '3', '9', 'Q', '(', '=', 'q', 'x', 'T', 'u', 'i', 'R', 'W', 'P', 'O', 'p', 'z', 'e', 'f', ',', 'D', 'J', '.', 'V', ':', '8', '2', 'L', 'o', '?', '5', '/', 'c', ')', 'H', 'l', '6', 'k', '&', '0', 'Y', 'G', 'h', 'A'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNq9Ktw_zrUf",
        "outputId": "316fd2b0-daf4-43df-8dbe-8a9565d83148"
      },
      "source": [
        "print('Total length: {}'.format(len(text)))\n",
        "print('Unique Characters: {}'.format(len(char_set)) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total length: 1112350\n",
            "Unique Characters: 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L0VzM1I1rSR",
        "outputId": "7aff569a-1621-4f6b-d2e1-18587f1134e7"
      },
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "print(chars_sorted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkNlbQg_2sTP",
        "outputId": "8ac40f7b-07c9-4123-a907-4456214972a6"
      },
      "source": [
        "char2int = {char:k for k,char in enumerate(chars_sorted)}\n",
        "print(char2int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '&': 3, '(': 4, ')': 5, '*': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '=': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '‘': 76, '’': 77, '“': 78, '”': 79}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyZJigmA6l84",
        "outputId": "35348974-afcc-43fb-a68e-a23c3982daa2"
      },
      "source": [
        "# chars_sorted is a list, now convert it into a char_array\n",
        "char_array = np.array(chars_sorted)\n",
        "print(char_array)\n",
        "print(char_array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '&' '(' ')' '*' ',' '-' '.' '/' '0' '1' '2' '3' '4' '5' '6'\n",
            " '7' '8' '9' ':' ';' '=' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K'\n",
            " 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z' 'a' 'b' 'c' 'd'\n",
            " 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v'\n",
            " 'w' 'x' 'y' 'z' '‘' '’' '“' '”']\n",
            "(80,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqv5-WBO9MDe",
        "outputId": "5c5a3d43-f7e7-4e2d-8544-8d33798b60ac"
      },
      "source": [
        "text_encoded = [ char2int[char] for char in text]\n",
        "text_encoded = np.array( text_encoded, dtype=np.int32 )\n",
        "print(text_encoded[:20])\n",
        "print(text_encoded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38]\n",
            "(1112350,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFTGMukv-iqk",
        "outputId": "a8fbd253-191e-4e56-89c9-2a65855fe3b5"
      },
      "source": [
        "# show some example\n",
        "print( '1. {} ---> {}'.format(text[:20], text_encoded[:20]) )\n",
        "print( '2. {} <--- {}'.format(text_encoded[15:21], text[15:21]) )\n",
        "print( '3. {} <--- {}'.format(text_encoded[15:21], char_array[text_encoded[15:21]] ) )\n",
        "print( '3\\'. {} <--- {}'.format(text_encoded[15:21], ''.join(char_array[text_encoded[15:21]]) ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. THE MYSTERIOUS ISLAN ---> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38]\n",
            "2. [33 43 36 25 38 28] <--- ISLAND\n",
            "3. [33 43 36 25 38 28] <--- ['I' 'S' 'L' 'A' 'N' 'D']\n",
            "3'. [33 43 36 25 38 28] <--- ISLAND\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bBAZuTUA5Me"
      },
      "source": [
        "The NumPy array text_encoded contains the encoded values for all the characters in the text. Now, we will create a TensorFlow dataset from this array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQ_Rdb_Bi9F",
        "outputId": "36c6b909-5616-4ad3-b8de-6eb31c5a2fac"
      },
      "source": [
        "# Create a source dataset from input data\n",
        "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "for element in dataset:\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JEoDbwFB2I5",
        "outputId": "ccf5e7ff-7bb5-4982-a189-ed7b22cc9689"
      },
      "source": [
        "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
        "for element in ds_text_encoded.take(5):\n",
        "  print(element)\n",
        "  print('{} -> {} \\n'.format( element.numpy(), char_array[element.numpy()] ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(44, shape=(), dtype=int32)\n",
            "44 -> T \n",
            "\n",
            "tf.Tensor(32, shape=(), dtype=int32)\n",
            "32 -> H \n",
            "\n",
            "tf.Tensor(29, shape=(), dtype=int32)\n",
            "29 -> E \n",
            "\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "1 ->   \n",
            "\n",
            "tf.Tensor(37, shape=(), dtype=int32)\n",
            "37 -> M \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ude21uhMUh0"
      },
      "source": [
        "To implement the text generation task in TensorFlow, let's first clip the sequence length to 40. This means that the input tensor, x, consists of 40 tokens. In practice, the sequence length impacts the quality of the generated text. Longer sequences can result in more meaningful sentences. For shorter sequences, however, the model might focus on capturing individual words correctly, while ignoring the context for the most part. Although longer sequences usually result in more meaningful sentences, as mentioned, for long sequences, the RNN model will have problems capturing long-term dependencies. Thus, in practice, finding a sweet spot and good value for the sequence length is a hyperparameter optimization problem, which we have to evaluate empirically. Here, we are going to choose 40, as it offers a good tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egXnZ61IMbmE"
      },
      "source": [
        "In order to predict the next character, the inputs, x, and targets, y, are offset by one character. Hence, we will split the text into chunks of size 41: the first 40 characters will form the input sequence, x, and the last 40 elements will form the target sequence, y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHZC8xYNOl6C"
      },
      "source": [
        "# Text chunks of 41 characters each\n",
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "ds_chunks  = ds_text_encoded.batch(chunk_size, drop_remainder=True) #  get rid of the last batch if it is shorter than 41 characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5hI0UtxPjmt",
        "outputId": "d7b34d0b-052e-4ffa-e522-5d72716bc27f"
      },
      "source": [
        "for chunk in ds_chunks.take(5):\n",
        "  print('{} - {} \\n'.format(chunk.shape, chunk.numpy()) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41,) - [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
            "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51 74] \n",
            "\n",
            "(41,) - [ 1 25 63 69 57 64 63 74  1 37 50 69 64 63 50 60  7  1 50 63 53  1 44 67\n",
            " 54 71 64 67  1 27 50 67 61 68 64 63  0  0  0  0  0] \n",
            "\n",
            "(41,) - [ 0 44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  0  0\n",
            " 51 74  1 34 70 61 54 68  1 46 54 67 63 54  0  0 12] \n",
            "\n",
            "(41,) - [19 18 15  0  0  0  0  0 40 25 42 44  1 12  8  8 28 42 39 40 40 29 28  1\n",
            " 30 42 39 37  1 44 32 29  1 27 36 39 45 28 43  0  0] \n",
            "\n",
            "(41,) - [ 0  0 27 57 50 65 69 54 67  1 12  0  0 78 25 67 54  1 72 54  1 67 58 68\n",
            " 58 63 56  1 50 56 50 58 63 24 79  1 78 38 64  9  1] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGp35FFqPWHP"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_seq  = chunk[:-1]\n",
        "  target_seq = chunk[1:]\n",
        "  return input_seq, target_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK5t9ToARjaD"
      },
      "source": [
        "ds_sequences = ds_chunks.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5mMIVn3_ofB",
        "outputId": "5ad024f1-b7a7-4a06-c80e-d135e6c5db99"
      },
      "source": [
        "for elem_input, elem_target in ds_sequences.take(2):\n",
        "  print('Input - X: {} - {}'.format( elem_input.numpy(), repr(''.join(char_array[elem_input])) ) )\n",
        "  print('Target - Y: {} - {}'.format( elem_target.numpy(), repr( ''.join(char_array[elem_target] )) ) )\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input - X: [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
            "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51] - 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
            "Target - Y: [32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6  6\n",
            "  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51 74] - 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "\n",
            "Input - X: [ 1 25 63 69 57 64 63 74  1 37 50 69 64 63 50 60  7  1 50 63 53  1 44 67\n",
            " 54 71 64 67  1 27 50 67 61 68 64 63  0  0  0  0] - ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\n",
            "Target - Y: [25 63 69 57 64 63 74  1 37 50 69 64 63 50 60  7  1 50 63 53  1 44 67 54\n",
            " 71 64 67  1 27 50 67 61 68 64 63  0  0  0  0  0] - 'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs2T_n0oCgBT"
      },
      "source": [
        "During the first preprocessing step to divide the dataset into batches, we created chunks of sentences. Each chunk represents one sentence, which corresponds to one training example. Now, we will shuffle the training examples and divide the inputs into mini-batches again; however, this time, each batch will contain multiple training examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRTQDE2jCiy6"
      },
      "source": [
        "BATCH_SIZE  = 64\n",
        "BUFFER_SIZE = 10_000\n",
        "ds          = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nejnd6RHDAG5"
      },
      "source": [
        "# Building a character-level RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaoDQoRwDGWJ"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=None),\n",
        "                               tf.keras.layers.LSTM(units=rnn_units, return_sequences=True),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPZ4sSR-GJBn",
        "outputId": "c6ac2c49-c194-4966-e222-8b745af2c8f7"
      },
      "source": [
        "## Setting the training parameters\n",
        "charset_size  = len(char_array) # 80 unique characters\n",
        "embedding_dim = 256\n",
        "rnn_units     = 512\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "model = build_model(vocab_size=charset_size, embedding_dim=embedding_dim, rnn_units=rnn_units)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 256)         20480     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 512)         1574912   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 80)          41040     \n",
            "=================================================================\n",
            "Total params: 1,636,432\n",
            "Trainable params: 1,636,432\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXgV2hVHRe8"
      },
      "source": [
        "Embedding layer : 80 * 256 = 20480 params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC9w2zNOHs3D"
      },
      "source": [
        "we specified activation=None for the final fully connected layer. The reason for this is that we will need to have the logits as outputs of the model so that we can sample from the model predictions in order to generate new text. We will get to this sampling part later. For now, let's train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-JCOzwuHroL",
        "outputId": "a920b531-7b69-4ad2-a2c9-ba89ee6e0781"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")\n",
        "\n",
        "model.fit(ds, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "424/424 [==============================] - 44s 33ms/step - loss: 2.7432\n",
            "Epoch 2/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.8257\n",
            "Epoch 3/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.5925\n",
            "Epoch 4/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.4641\n",
            "Epoch 5/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.3802\n",
            "Epoch 6/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.3264\n",
            "Epoch 7/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.2831\n",
            "Epoch 8/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.2502\n",
            "Epoch 9/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.2208\n",
            "Epoch 10/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.1997\n",
            "Epoch 11/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.1763\n",
            "Epoch 12/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.1582\n",
            "Epoch 13/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.1420\n",
            "Epoch 14/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.1268\n",
            "Epoch 15/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.1115\n",
            "Epoch 16/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.0982\n",
            "Epoch 17/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.0861\n",
            "Epoch 18/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.0702\n",
            "Epoch 19/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.0594\n",
            "Epoch 20/20\n",
            "424/424 [==============================] - 15s 33ms/step - loss: 1.0462\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0ae009f350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1SJer2JIER"
      },
      "source": [
        "# Evaluation phase – generating new text passages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzwet9vRJhYV"
      },
      "source": [
        "The RNN model we trained in the previous section returns the logits of size 80 for each unique character. These logits can be readily converted to probabilities, via the softmax function, that a particular character will be encountered as the next character. To predict the next character in the sequence, we can simply select the element with the maximum logit value, which is equivalent to selecting the character with the highest probability. However, instead of always selecting the character with the highest likelihood, we want to (randomly) sample from the outputs; otherwise, the model will always produce the same text. TensorFlow already provides a function, tf.random.categorical(), which we can use to draw random samples from a categorical distribution. To see how this works, let's generate some random samples from three categories [0, 1, 2], with input logits [1, 1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIgmRoYxJvjk",
        "outputId": "7148aba7-ba72-4790-ec72-9dd943f1f98a"
      },
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "logits = [[1.0, 1.0, 1.0]]\n",
        "print('Probabilities:', tf.math.softmax(logits).numpy()[0])\n",
        "\n",
        "samples = tf.random.categorical(logits=logits, num_samples=10)\n",
        "tf.print(samples.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probabilities: [0.33333334 0.33333334 0.33333334]\n",
            "array([[1, 2, 0, 1, 0, 1, 1, 2, 1, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWsXpB2XKA0Y",
        "outputId": "3abea2fc-5179-48f5-c65d-a4cf5c4a4294"
      },
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "logits = [[1.0, 1.0, 3.0]]\n",
        "print('Probabilities:', tf.math.softmax(logits).numpy()[0])\n",
        "\n",
        "samples = tf.random.categorical(logits=logits, num_samples=11)\n",
        "tf.print(samples.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probabilities: [0.10650698 0.10650698 0.78698605]\n",
            "array([[2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iZ4upbiMzlt"
      },
      "source": [
        "def sample(model, starting_str, len_generated_text=500, max_input_length=40, scale_factor=1.):\n",
        "  encoded_input = [char2int[x] for x in starting_str]\n",
        "  encoded_input = tf.reshape( encoded_input, (1, -1) ) # 1 row, n cols\n",
        "\n",
        "  generated_str = starting_str\n",
        "\n",
        "  model.reset_states() # reset the states of all layers in the model\n",
        "\n",
        "  for i in range(len_generated_text):\n",
        "    logits = model(encoded_input)\n",
        "    logits = tf.squeeze(logits, axis=0) # Removes dimensions of size 1 from the shape of a tensor. [1, 2, 1, 3, 1, 1] -> [2, 3]\n",
        "    \n",
        "    scaled_logits = scale_factor * logits\n",
        "\n",
        "    new_char_idx = tf.random.categorical(scaled_logits, num_samples=1)\n",
        "    new_char_idx = tf.squeeze(new_char_idx)[-1].numpy()\n",
        "    \n",
        "    # Add new character to the end of the string\n",
        "    generated_str += str( char_array[new_char_idx] )\n",
        "\n",
        "    # Add the new character_index to the end of the encoded_input\n",
        "    new_char_idx = tf.expand_dims([new_char_idx], 0) # Returns a tensor with a length 1 axis inserted at index axis. e.g [10,10,3] - > [1, 10,10,3]\n",
        "\n",
        "    encoded_input = tf.concat([encoded_input, new_char_idx], axis=1)\n",
        "    encoded_input = encoded_input[:, -max_input_length : ] # just take the last 40 characters\n",
        "\n",
        "  return generated_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiK6C8t9RLzB",
        "outputId": "14708c55-7bc9-4b66-a880-0e70eee8056d"
      },
      "source": [
        "tf.random.set_seed(1)\n",
        "print(sample(model, starting_str='The island'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The island was explored their winding\n",
            "colony. The “Nautilus” making the equarte with those oceand.\n",
            "\n",
            "“Who can certainly no venture or Patalunation of the wild beasts. The well worth inable met himself prolonged\n",
            "coulls, were save\n",
            "you intoly the convicts be a subterranean solour’s border! The wind continued felt from him long in Llacking but\n",
            "the colonists felt.\n",
            "\n",
            "“But do you reach Pencroft’s gining len.”\n",
            "\n",
            "“Inabletted all the gulleys must any event from each penewer at the wire towards the last, some sign of i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4IyyqyWTU9x"
      },
      "source": [
        "## Predictability vs. randomness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHxzEUJ-Tl4g"
      },
      "source": [
        "By scaling the logits with a factor < 1, the probabilities computed by the softmax function become more uniform, as shown in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sOX73d0TUWj",
        "outputId": "ff8d8fde-ce26-4063-a574-16f5803b9117"
      },
      "source": [
        "logits = np.array([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('Probabilities before scaling:        ', tf.math.softmax(logits).numpy()[0])\n",
        "\n",
        "print('Probabilities after scaling with 0.5:', tf.math.softmax(0.5*logits).numpy()[0])\n",
        "\n",
        "print('Probabilities after scaling with 0.1:', tf.math.softmax(0.1*logits).numpy()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probabilities before scaling:         [0.10650698 0.10650698 0.78698604]\n",
            "Probabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611688]\n",
            "Probabilities after scaling with 0.1: [0.31042377 0.31042377 0.37915245]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVo3xfH8Tx5w"
      },
      "source": [
        "Scaling the logits by alpha=0.1 results in near-uniform probabilities [0.31, 0.31, 0.38]. Now, we can compare the generated text with alpha=2.0 (more predictable) and alpha=0.5 (more randomness)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jgjkXnfTWJH",
        "outputId": "8b25709e-7f63-4161-da55-3e1deccc63a6"
      },
      "source": [
        "tf.random.set_seed(1)\n",
        "print(sample(model, starting_str='The island', scale_factor=2.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The island was extended to the faithful animals of a second coast. The most recent latitude was advice, and the sudden was still provisions and soon set out of the state of the Prince Dakkar Grotto, and his companions were so completely considered it to be the corral. The colonists had a second contingent power of the lake was lighted by the colonists still being so strict in the river which he had destroyed their companions to extinguish the pirates were thrown upon the stranger at the same time at the s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-3dQqZ1UJuL",
        "outputId": "471dd71a-418a-4bff-e267-b0f13cbd8f93"
      },
      "source": [
        "tf.random.set_seed(1)\n",
        "print(sample(model, starting_str='The island', scale_factor=0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The island was egely place to kinny wh float! Well, that?”\n",
            "-Chackyan\n",
            "24me, Harding,--tho “and that is go favoces.” observed Evidepth Pacolur. Rown, Spilett, repained any myn inder-siea.” Ame of Migro’!\n",
            "\n",
            "“We can, atlispher 8\n",
            "she\n",
            "you “yell speak,” said he.\n",
            " Top, this inglittod and push, making. Twen their voints next to lort. Carlingullock bodne of\n",
            "Elunis, smeling, whnee\n",
            "war asily from\n",
            "the pirate’c dwelbing!\n",
            "\n",
            "To:\n",
            "illy ourselvely yeck,” anseed,\n",
            "lift; vercy, linrers-fill, with under Latienly frisonmen\n",
            "to puri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fL8rWqrUWlV"
      },
      "source": [
        "The results show that scaling the logits with  alpha=0.5 (similar to increasing the temperature) generates more random text. There is a tradeoff between the novelty of the generated text and its correctness."
      ]
    }
  ]
}